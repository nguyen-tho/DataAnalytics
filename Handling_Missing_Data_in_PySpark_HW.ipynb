{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Data in PySpark HW Solutions\n",
    "\n",
    "In this HW assignment you will be strengthening your skill sets dealing with missing data.\n",
    " \n",
    "**Review:** you have 2 basic options for filling in missing data (you will personally have to make the decision for what is the right approach:\n",
    "\n",
    "1. Drop the missing data points (including the entire row)\n",
    "2. Fill them in with some other value.\n",
    "\n",
    "Let's practice some examples of each of these methods!\n",
    "\n",
    "\n",
    "#### But first!\n",
    "\n",
    "Start your Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder.appName('handling').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the dataset for this Notebook\n",
    "\n",
    "Weather.csv attached to this lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path ='dataset/Weather.csv'\n",
    "\n",
    "data = spark.read.csv(dataset_path,inferSchema=True,header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this dataset\n",
    "\n",
    "**New York City Taxi Trip - Hourly Weather Data**\n",
    "\n",
    "Here is some detailed weather data for the New York City Taxi Trips.\n",
    "\n",
    "**Source:** https://www.kaggle.com/meinertsen/new-york-city-taxi-trip-hourly-weather-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print a view of the first several lines of the dataframe to see what our data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+------+------+----+-----+-----+------+------+-----+--------+----+----+---------+---------+----------+----------+----------+----------+-------+-------+----------------+------------+---+----+----+----+-------+-------+\n",
      "|    pickup_datetime|tempm|tempi|dewptm|dewpti| hum|wspdm|wspdi|wgustm|wgusti|wdird|   wdire|vism|visi|pressurem|pressurei|windchillm|windchilli|heatindexm|heatindexi|precipm|precipi|           conds|        icon|fog|rain|snow|hail|thunder|tornado|\n",
      "+-------------------+-----+-----+------+------+----+-----+-----+------+------+-----+--------+----+----+---------+---------+----------+----------+----------+----------+-------+-------+----------------+------------+---+----+----+----+-------+-------+\n",
      "|2015-12-31 00:15:00|  7.8| 46.0|   6.1|  43.0|89.0|  7.4|  4.6|  NULL|  NULL|   40|      NE| 4.0| 2.5|   1018.2|    30.07|       6.6|      43.9|      NULL|      NULL|    0.5|   0.02|      Light Rain|        rain|  0|   1|   0|   0|      0|      0|\n",
      "|2015-12-31 00:42:00|  7.8| 46.0|   6.1|  43.0|89.0|  7.4|  4.6|  NULL|  NULL|    0|Variable| 6.4| 4.0|   1017.8|    30.06|       6.6|      43.9|      NULL|      NULL|    0.8|   0.03|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 00:51:00|  7.8| 46.0|   6.1|  43.0|89.0|  5.6|  3.5|  NULL|  NULL|   20|     NNE| 8.0| 5.0|   1017.0|    30.04|       7.1|      44.8|      NULL|      NULL|    0.8|   0.03|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 01:51:00|  7.2| 45.0|   5.6|  42.1|90.0|  7.4|  4.6|  NULL|  NULL|    0|Variable|12.9| 8.0|   1016.5|    30.02|       5.9|      42.6|      NULL|      NULL|    0.3|   0.01|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 02:51:00|  7.2| 45.0|   5.6|  42.1|90.0|  0.0|  0.0|  NULL|  NULL|    0|   North|12.9| 8.0|   1016.7|    30.03|      NULL|      NULL|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 03:28:00|  6.7| 44.1|   5.0|  41.0|89.0|  7.4|  4.6|  NULL|  NULL|  300|     WNW|12.9| 8.0|   1017.2|    30.04|       5.3|      41.5|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 03:40:00|  7.2| 45.0|   5.0|  41.0|86.0|  0.0|  0.0|  NULL|  NULL|    0|   North|12.9| 8.0|   1016.8|    30.03|      NULL|      NULL|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 03:51:00|  7.2| 45.0|   5.0|  41.0|86.0|  7.4|  4.6|  NULL|  NULL|  310|      NW|14.5| 9.0|   1015.9|     30.0|       5.9|      42.6|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 04:22:00|  7.2| 45.0|   5.0|  41.0|86.0|  5.6|  3.5|  NULL|  NULL|  280|    West|14.5| 9.0|   1016.8|    30.03|       6.4|      43.5|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 04:51:00|  7.2| 45.0|   5.6|  42.1|90.0|  5.6|  3.5|  NULL|  NULL|  270|    West|11.3| 7.0|   1016.2|    30.01|       6.4|      43.5|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 05:14:00|  7.2| 45.0|   5.0|  41.0|86.0|  0.0|  0.0|  NULL|  NULL|    0|   North|12.9| 8.0|   1017.2|    30.04|      NULL|      NULL|      NULL|      NULL|   NULL|   NULL|Scattered Clouds|partlycloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 05:51:00|  7.8| 46.0|   5.6|  42.1|86.0|  5.6|  3.5|  NULL|  NULL|    0|Variable|16.1|10.0|   1016.6|    30.02|       7.1|      44.8|      NULL|      NULL|   NULL|   NULL|           Clear|       clear|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 06:51:00|  7.8| 46.0|   5.6|  42.1|86.0|  7.4|  4.6|  NULL|  NULL|    0|Variable|16.1|10.0|   1016.9|    30.03|       6.6|      43.9|      NULL|      NULL|   NULL|   NULL|           Clear|       clear|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 07:14:00|  7.8| 46.0|   5.6|  42.1|86.0|  7.4|  4.6|  NULL|  NULL|  270|    West|14.5| 9.0|   1018.2|    30.07|       6.6|      43.9|      NULL|      NULL|   NULL|   NULL|   Mostly Cloudy|mostlycloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 07:28:00|  8.3| 46.9|   5.6|  42.1|83.0|  7.4|  4.6|  NULL|  NULL|  290|     WNW|16.1|10.0|   1018.2|    30.07|      NULL|      NULL|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 07:51:00|  8.3| 46.9|   5.6|  42.1|83.0|  5.6|  3.5|  NULL|  NULL|  290|     WNW|14.5| 9.0|   1017.7|    30.06|      NULL|      NULL|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 08:51:00|  8.3| 46.9|   5.0|  41.0|80.0|  7.4|  4.6|  NULL|  NULL|    0|Variable|16.1|10.0|   1018.5|    30.08|      NULL|      NULL|      NULL|      NULL|   NULL|   NULL|           Clear|       clear|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 09:51:00|  8.3| 46.9|   2.8|  37.0|68.0|  9.3|  5.8|  NULL|  NULL|  300|     WNW|16.1|10.0|   1019.0|    30.09|      NULL|      NULL|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 10:51:00|  8.3| 46.9|   2.2|  36.0|66.0| 16.7| 10.4|  NULL|  NULL|  310|      NW|16.1|10.0|   1018.9|    30.09|      NULL|      NULL|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 11:51:00|  8.3| 46.9|   1.1|  34.0|61.0| 14.8|  9.2|  29.6|  18.4|  310|      NW|16.1|10.0|   1017.7|    30.06|      NULL|      NULL|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "+-------------------+-----+-----+------+------+----+-----+-----+------+------+-----+--------+----+----+---------+---------+----------+----------+----------+----------+-------+-------+----------------+------------+---+----+----+----+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the schema \n",
    "\n",
    "So that we can see if we need to make any corrections to the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- tempm: double (nullable = true)\n",
      " |-- tempi: double (nullable = true)\n",
      " |-- dewptm: double (nullable = true)\n",
      " |-- dewpti: double (nullable = true)\n",
      " |-- hum: double (nullable = true)\n",
      " |-- wspdm: double (nullable = true)\n",
      " |-- wspdi: double (nullable = true)\n",
      " |-- wgustm: double (nullable = true)\n",
      " |-- wgusti: double (nullable = true)\n",
      " |-- wdird: integer (nullable = true)\n",
      " |-- wdire: string (nullable = true)\n",
      " |-- vism: double (nullable = true)\n",
      " |-- visi: double (nullable = true)\n",
      " |-- pressurem: double (nullable = true)\n",
      " |-- pressurei: double (nullable = true)\n",
      " |-- windchillm: double (nullable = true)\n",
      " |-- windchilli: double (nullable = true)\n",
      " |-- heatindexm: double (nullable = true)\n",
      " |-- heatindexi: double (nullable = true)\n",
      " |-- precipm: double (nullable = true)\n",
      " |-- precipi: double (nullable = true)\n",
      " |-- conds: string (nullable = true)\n",
      " |-- icon: string (nullable = true)\n",
      " |-- fog: integer (nullable = true)\n",
      " |-- rain: integer (nullable = true)\n",
      " |-- snow: integer (nullable = true)\n",
      " |-- hail: integer (nullable = true)\n",
      " |-- thunder: integer (nullable = true)\n",
      " |-- tornado: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How much missing data are we working with?\n",
    "\n",
    "Get a count and percentage of each variable in the dataset to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+-------------------+\n",
      "|Column_Name|Null_Values_Count| Null_Value_Percent|\n",
      "+-----------+-----------------+-------------------+\n",
      "|      tempm|                5|0.04770537162484496|\n",
      "|      tempi|                5|0.04770537162484496|\n",
      "|     dewptm|                5|0.04770537162484496|\n",
      "|     dewpti|                5|0.04770537162484496|\n",
      "|        hum|                5|0.04770537162484496|\n",
      "|      wspdm|              737|  7.031771777502146|\n",
      "|      wspdi|              737|  7.031771777502146|\n",
      "|     wgustm|             8605|  82.10094456635817|\n",
      "|     wgusti|             8605|  82.10094456635817|\n",
      "|       vism|              245| 2.3375632096174033|\n",
      "|       visi|              245| 2.3375632096174033|\n",
      "|  pressurem|              239| 2.2803167636675887|\n",
      "|  pressurei|              239| 2.2803167636675887|\n",
      "| windchillm|             7775|  74.18185287663391|\n",
      "| windchilli|             7775|  74.18185287663391|\n",
      "| heatindexm|             9644|  92.01412079000096|\n",
      "| heatindexi|             9644|  92.01412079000096|\n",
      "|    precipm|             8775|   83.7229272016029|\n",
      "|    precipi|             8775|   83.7229272016029|\n",
      "+-----------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "def null_value_calc(df):\n",
    "    null_columns_counts = []\n",
    "    numRows = df.count()\n",
    "    for k in df.columns:\n",
    "        nullRows = df.where(col(k).isNull()).count()\n",
    "        if(nullRows > 0):\n",
    "            temp = k,nullRows,(nullRows/numRows)*100\n",
    "            null_columns_counts.append(temp)\n",
    "    return(null_columns_counts)\n",
    "\n",
    "null_columns_calc_list = null_value_calc(data)\n",
    "null_df = spark.createDataFrame(null_columns_calc_list, ['Column_Name', 'Null_Values_Count','Null_Value_Percent'])\n",
    "null_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How many rows contain at least one null value?\n",
    "\n",
    "We want to know, if we use the df.na option, how many rows will we loose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with at least one null value: 10481\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "filtered_df = data.filter(F.col(\"pickup_datetime\").isNull() | F.col(\"tempm\").isNull() |\n",
    "                          F.col(\"tempi\").isNull() | F.col(\"dewptm\").isNull() | F.col(\"dewpti\").isNull() |\n",
    "                          F.col(\"hum\").isNull() | F.col(\"wspdm\").isNull() | F.col(\"wspdi\").isNull() |\n",
    "                          F.col(\"wgustm\").isNull() | F.col(\"wgusti\").isNull() | F.col(\"wdird\").isNull() | \n",
    "                          F.col(\"wdire\").isNull() | F.col(\"vism\").isNull() | F.col(\"visi\").isNull() | \n",
    "                          F.col(\"pressurem\").isNull() | F.col(\"pressurei\").isNull() | \n",
    "                          F.col(\"windchillm\").isNull() | F.col(\"windchilli\").isNull() | \n",
    "                          F.col(\"heatindexm\").isNull() | F.col(\"heatindexi\").isNull() | \n",
    "                          F.col(\"precipm\").isNull() | F.col(\"precipi\").isNull() | F.col(\"conds\").isNull() | \n",
    "                          F.col(\"icon\").isNull() | F.col(\"fog\").isNull() | F.col(\"rain\").isNull() | F.col(\"snow\").isNull() | \n",
    "                          F.col(\"hail\").isNull() | F.col(\"thunder\").isNull() | F.col(\"tornado\").isNull())\n",
    "\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "#filtered_df = data.filter([data[c].isNull() for c in data.columns])\n",
    "\n",
    "# Count the number of rows in the filtered DataFrame\n",
    "num_rows_with_nulls = filtered_df.count()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of rows with at least one null value: {num_rows_with_nulls}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Drop the missing data\n",
    "\n",
    "Drop any row that contains missing data across the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+-----+------+------+---+-----+-----+------+------+-----+-----+----+----+---------+---------+----------+----------+----------+----------+-------+-------+-----+----+---+----+----+----+-------+-------+\n",
      "|pickup_datetime|tempm|tempi|dewptm|dewpti|hum|wspdm|wspdi|wgustm|wgusti|wdird|wdire|vism|visi|pressurem|pressurei|windchillm|windchilli|heatindexm|heatindexi|precipm|precipi|conds|icon|fog|rain|snow|hail|thunder|tornado|\n",
      "+---------------+-----+-----+------+------+---+-----+-----+------+------+-----+-----+----+----+---------+---------+----------+----------+----------+----------+-------+-------+-----+----+---+----+----+----+-------+-------+\n",
      "+---------------+-----+-----+------+------+---+-----+-----+------+------+-----+-----+----+----+---------+---------+----------+----------+----------+----------+-------+-------+-----+----+---+----+----+----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drop_missing_data = data.na.drop()\n",
    "drop_missing_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Drop with a threshold\n",
    "\n",
    "Count how many rows would be dropped if we only dropped rows that had a least 12 NON-Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows Dropped: 5\n"
     ]
    }
   ],
   "source": [
    "og_len = data.count()\n",
    "drop_len = data.na.drop(thresh=12).count()\n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Drop rows according to specific column value\n",
    "\n",
    "Now count how many rows would be dropped if you only drop rows whose values in the tempm column are null/NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows to be dropped: 5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "filtered_df = data.filter(F.col(\"tempm\").isNotNull())  # Keep rows with non-null tempm\n",
    "\n",
    "# Count the number of rows in the original DataFrame\n",
    "total_rows = data.count()\n",
    "\n",
    "# Count the number of rows remaining after filtering\n",
    "remaining_rows = filtered_df.count()\n",
    "\n",
    "# Calculate the number of rows that would be dropped\n",
    "rows_to_drop = total_rows - remaining_rows\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of rows to be dropped: {rows_to_drop}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Drop rows that are null accross all columns\n",
    "\n",
    "Count how many rows would be dropped if you only dropped rows where ALL the values are null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows dropped if only rows with all null values are removed: 10481\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "# Filter rows where none of the columns are null\n",
    "all_columns_not_null = data.where(~F.col(\"pickup_datetime\").isNull())\n",
    "\n",
    "for col in data.columns:\n",
    "  all_columns_not_null = all_columns_not_null.where(~F.col(col).isNull())\n",
    "\n",
    "# Count the number of rows that would be dropped (total rows - rows with non-null values)\n",
    "num_dropped_rows = data.count() - all_columns_not_null.count()\n",
    "\n",
    "# Print the number of dropped rows\n",
    "print(f\"Number of rows dropped if only rows with all null values are removed: {num_dropped_rows}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fill in all the string columns missing values with the word \"N/A\"\n",
    "\n",
    "Make sure you don't edit the datadataframe itself. Create a copy of the datathen edit that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+------+------+----+-----+-----+------+------+-----+--------+----+----+---------+---------+----------+----------+----------+----------+-------+-------+----------------+------------+---+----+----+----+-------+-------+\n",
      "|    pickup_datetime|tempm|tempi|dewptm|dewpti| hum|wspdm|wspdi|wgustm|wgusti|wdird|   wdire|vism|visi|pressurem|pressurei|windchillm|windchilli|heatindexm|heatindexi|precipm|precipi|           conds|        icon|fog|rain|snow|hail|thunder|tornado|\n",
      "+-------------------+-----+-----+------+------+----+-----+-----+------+------+-----+--------+----+----+---------+---------+----------+----------+----------+----------+-------+-------+----------------+------------+---+----+----+----+-------+-------+\n",
      "|2015-12-31 00:15:00|  7.8| 46.0|   6.1|  43.0|89.0|  7.4|  4.6|  NULL|  NULL|   40|      NE| 4.0| 2.5|   1018.2|    30.07|       6.6|      43.9|      NULL|      NULL|    0.5|   0.02|      Light Rain|        rain|  0|   1|   0|   0|      0|      0|\n",
      "|2015-12-31 00:42:00|  7.8| 46.0|   6.1|  43.0|89.0|  7.4|  4.6|  NULL|  NULL|    0|Variable| 6.4| 4.0|   1017.8|    30.06|       6.6|      43.9|      NULL|      NULL|    0.8|   0.03|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 00:51:00|  7.8| 46.0|   6.1|  43.0|89.0|  5.6|  3.5|  NULL|  NULL|   20|     NNE| 8.0| 5.0|   1017.0|    30.04|       7.1|      44.8|      NULL|      NULL|    0.8|   0.03|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 01:51:00|  7.2| 45.0|   5.6|  42.1|90.0|  7.4|  4.6|  NULL|  NULL|    0|Variable|12.9| 8.0|   1016.5|    30.02|       5.9|      42.6|      NULL|      NULL|    0.3|   0.01|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 02:51:00|  7.2| 45.0|   5.6|  42.1|90.0|  0.0|  0.0|  NULL|  NULL|    0|   North|12.9| 8.0|   1016.7|    30.03|      NULL|      NULL|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 03:28:00|  6.7| 44.1|   5.0|  41.0|89.0|  7.4|  4.6|  NULL|  NULL|  300|     WNW|12.9| 8.0|   1017.2|    30.04|       5.3|      41.5|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 03:40:00|  7.2| 45.0|   5.0|  41.0|86.0|  0.0|  0.0|  NULL|  NULL|    0|   North|12.9| 8.0|   1016.8|    30.03|      NULL|      NULL|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 03:51:00|  7.2| 45.0|   5.0|  41.0|86.0|  7.4|  4.6|  NULL|  NULL|  310|      NW|14.5| 9.0|   1015.9|     30.0|       5.9|      42.6|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 04:22:00|  7.2| 45.0|   5.0|  41.0|86.0|  5.6|  3.5|  NULL|  NULL|  280|    West|14.5| 9.0|   1016.8|    30.03|       6.4|      43.5|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 04:51:00|  7.2| 45.0|   5.6|  42.1|90.0|  5.6|  3.5|  NULL|  NULL|  270|    West|11.3| 7.0|   1016.2|    30.01|       6.4|      43.5|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 05:14:00|  7.2| 45.0|   5.0|  41.0|86.0|  0.0|  0.0|  NULL|  NULL|    0|   North|12.9| 8.0|   1017.2|    30.04|      NULL|      NULL|      NULL|      NULL|   NULL|   NULL|Scattered Clouds|partlycloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 05:51:00|  7.8| 46.0|   5.6|  42.1|86.0|  5.6|  3.5|  NULL|  NULL|    0|Variable|16.1|10.0|   1016.6|    30.02|       7.1|      44.8|      NULL|      NULL|   NULL|   NULL|           Clear|       clear|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 06:51:00|  7.8| 46.0|   5.6|  42.1|86.0|  7.4|  4.6|  NULL|  NULL|    0|Variable|16.1|10.0|   1016.9|    30.03|       6.6|      43.9|      NULL|      NULL|   NULL|   NULL|           Clear|       clear|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 07:14:00|  7.8| 46.0|   5.6|  42.1|86.0|  7.4|  4.6|  NULL|  NULL|  270|    West|14.5| 9.0|   1018.2|    30.07|       6.6|      43.9|      NULL|      NULL|   NULL|   NULL|   Mostly Cloudy|mostlycloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 07:28:00|  8.3| 46.9|   5.6|  42.1|83.0|  7.4|  4.6|  NULL|  NULL|  290|     WNW|16.1|10.0|   1018.2|    30.07|      NULL|      NULL|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 07:51:00|  8.3| 46.9|   5.6|  42.1|83.0|  5.6|  3.5|  NULL|  NULL|  290|     WNW|14.5| 9.0|   1017.7|    30.06|      NULL|      NULL|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 08:51:00|  8.3| 46.9|   5.0|  41.0|80.0|  7.4|  4.6|  NULL|  NULL|    0|Variable|16.1|10.0|   1018.5|    30.08|      NULL|      NULL|      NULL|      NULL|   NULL|   NULL|           Clear|       clear|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 09:51:00|  8.3| 46.9|   2.8|  37.0|68.0|  9.3|  5.8|  NULL|  NULL|  300|     WNW|16.1|10.0|   1019.0|    30.09|      NULL|      NULL|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 10:51:00|  8.3| 46.9|   2.2|  36.0|66.0| 16.7| 10.4|  NULL|  NULL|  310|      NW|16.1|10.0|   1018.9|    30.09|      NULL|      NULL|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "|2015-12-31 11:51:00|  8.3| 46.9|   1.1|  34.0|61.0| 14.8|  9.2|  29.6|  18.4|  310|      NW|16.1|10.0|   1017.7|    30.06|      NULL|      NULL|      NULL|      NULL|   NULL|   NULL|        Overcast|      cloudy|  0|   0|   0|   0|      0|      0|\n",
      "+-------------------+-----+-----+------+------+----+-----+-----+------+------+-----+--------+----+----+---------+---------+----------+----------+----------+----------+-------+-------+----------------+------------+---+----+----+----+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "# Select only string columns (filter data types)\n",
    "#string_cols = data.select(data.filter(F.col(\"dataType\").cast(\"string\").contains(\"string\")))\n",
    "\n",
    "# Impute missing values in string columns with 'N/A'\n",
    "imputed_df = data.fillna('N/A')\n",
    "\n",
    "# Create a new DataFrame with all columns by joining the imputed string columns with the original DataFrame\n",
    "#all_cols_df = data.join(imputed_df, how='left')\n",
    "\n",
    "imputed_df.show()\n",
    "# Print the original and imputed DataFrames (optional)\n",
    "# print(\"Original DataFrame:\\n\", df.show())\n",
    "# print(\"Imputed DataFrame:\\n\", all_cols_df.show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fill in NaN values with averages for the tempm and tempi columns\n",
    "\n",
    "*Note: you will first need to compute the averages for each column and then fill in with the corresponding value.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: 13.705622374952132 of type <class 'float'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m avg_temp_i \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39magg(F\u001b[38;5;241m.\u001b[39mavg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtempi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_tempi_i\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Fill NaN values in tempm and tempi with the respective averages\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m filled_df \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtempm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtempm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg_temp_m\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mavg_tempm_m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m) \\\n\u001b[0;32m     10\u001b[0m              \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtempi\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mcoalesce(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtempi\u001b[39m\u001b[38;5;124m\"\u001b[39m), avg_temp_i\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_tempi_i\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfirst()[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Print the original and filled DataFrames (optional)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# print(\"Original DataFrame:\\n\", df.show())\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFilled DataFrame:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, filled_df\u001b[38;5;241m.\u001b[39mshow())\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\pyspark-demo\\Lib\\site-packages\\pyspark\\sql\\utils.py:160\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\pyspark-demo\\Lib\\site-packages\\pyspark\\sql\\functions.py:2738\u001b[0m, in \u001b[0;36mcoalesce\u001b[1;34m(*cols)\u001b[0m\n\u001b[0;32m   2689\u001b[0m \u001b[38;5;129m@try_remote_functions\u001b[39m\n\u001b[0;32m   2690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcoalesce\u001b[39m(\u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[0;32m   2691\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the first column that is not null.\u001b[39;00m\n\u001b[0;32m   2692\u001b[0m \n\u001b[0;32m   2693\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.4.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2736\u001b[0m \u001b[38;5;124;03m    +----+----+----------------+\u001b[39;00m\n\u001b[0;32m   2737\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_invoke_function_over_seq_of_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoalesce\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\pyspark-demo\\Lib\\site-packages\\pyspark\\sql\\functions.py:112\u001b[0m, in \u001b[0;36m_invoke_function_over_seq_of_columns\u001b[1;34m(name, cols)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03mInvokes unary JVM function identified by name with\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03mand wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m sc \u001b[38;5;241m=\u001b[39m get_active_spark_context()\n\u001b[1;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(name, \u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_java_column\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\pyspark-demo\\Lib\\site-packages\\pyspark\\sql\\column.py:86\u001b[0m, in \u001b[0;36m_to_seq\u001b[1;34m(sc, cols, converter)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03minto JVM Column objects.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[1;32m---> 86\u001b[0m     cols \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(cols)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\pyspark-demo\\Lib\\site-packages\\pyspark\\sql\\column.py:86\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03minto JVM Column objects.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[1;32m---> 86\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [\u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(cols)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\pyspark-demo\\Lib\\site-packages\\pyspark\\sql\\column.py:65\u001b[0m, in \u001b[0;36m_to_java_column\u001b[1;34m(col)\u001b[0m\n\u001b[0;32m     63\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m _create_column_from_name(col)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid argument, not a string or column: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor column literals, use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstruct\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreate_map\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(col, \u001b[38;5;28mtype\u001b[39m(col))\n\u001b[0;32m     70\u001b[0m     )\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jcol\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid argument, not a string or column: 13.705622374952132 of type <class 'float'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "# Calculate the average temperature for tempm and tempi columns\n",
    "avg_temp_m = data.agg(F.avg(\"tempm\").alias(\"avg_tempm_m\"))\n",
    "avg_temp_i = data.agg(F.avg(\"tempi\").alias(\"avg_tempi_i\"))\n",
    "\n",
    "# Fill NaN values in tempm and tempi with the respective averages\n",
    "filled_df = data.withColumn(\"tempm\", F.coalesce(F.col(\"tempm\"), avg_temp_m.select(\"avg_tempm_m\").first()[0])) \\\n",
    "             .withColumn(\"tempi\", F.coalesce(F.col(\"tempi\"), avg_temp_i.select(\"avg_tempi_i\").first()[0]))\n",
    "\n",
    "# Print the original and filled DataFrames (optional)\n",
    "# print(\"Original DataFrame:\\n\", df.show())\n",
    "print(\"Filled DataFrame:\\n\", filled_df.show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's it! Great Job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
